[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R/Pharma Summit",
    "section": "",
    "text": "In 2018 and 2019, R in Pharma was in-person at Harvard University and focused on opportunities for direct interaction with speakers and guests.\nThese relationships and connections have grown into many exciting areas of Open Source drug development.\n\nIn 2023 we partnered with Posit, hosting an in-person session at posit::conf(2023) in Chicago.\n\nIn 2024, the session continued at posit::conf(2024) in Seattle.\n\nWe‚Äôre thrilled to bring this back at posit::conf(2025) in Atlanta! üéâ\n\n\nOver the last five years we‚Äôve seen an explosive growth in the use of R and other open source technologies across drug development, with an increasing focus on pan-company collaboration.\nThe R/Pharma Roundtable Summit provides an open, collaborative, and inclusive environment to:\n\nShare learnings\n\nUnderstand common themes across our industry\n\nEstablish collaboration opportunities\n\nThe focus is to foster in-person discussions about key items such as reproducibility and submissions, much like the original Harvard events.\n\nüîó Review the 2023‚Äì2024 roundtable summaries here:\nrinpharma.github.io/roundtables\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can contribute to the discussion about the final agenda!\nJoin the GitHub discussions here:\nüëâ rinpharma-summit-2025 Discussions"
  },
  {
    "objectID": "index.html#call-for-input",
    "href": "index.html#call-for-input",
    "title": "R/Pharma Summit",
    "section": "",
    "text": "Note\n\n\n\nYou can contribute to the discussion about the final agenda!\nJoin the GitHub discussions here:\nüëâ rinpharma-summit-2025 Discussions"
  },
  {
    "objectID": "index.html#roundtables",
    "href": "index.html#roundtables",
    "title": "R/Pharma Summit",
    "section": "Roundtables",
    "text": "Roundtables\n\n1. AI in Pharma\n\nChairs: Jeremy Wildfire, Sri Pavan Vemuri, Satish Murthy\n\nBest Practices for AI-Assisted R Coding\n\nWhat would an ‚ÄúAI-friendly‚Äù submission look like?\n\nReimagine open-label trial deliverables with interactive display/AI\n\nBest Practices for AI Integration in Data Workflows\n\nCreate more tables if needed for GenAI\n\n\n\n\n2. Validation in 2025\n\nChairs: Olga Mierzwa-Sulima, Eric Nantz, Paulo Bargo\n\nValidating Open Source Tools with Stochastic Components\n\nArchitectures to provision validated R packages\n\n\n\n\n3. Python for Clinical Study Reports and Submission\n\nChairs: Yilong Zhang, Jonathan Tisack\n\nMedical Devices\n\n\n\n\n4. SCEs in 2025\n\nChairs: David Thiriot, Phil Bowsher, James Black\n\nOpen-source infrastructure to complement openstatsware and the pharmaverse\n\nExpanding the Pharmaverse beyond Clinical Reporting\n\n\n\n\n5. Open-Source Change Management\n\nChairs: Ning Leng, Nate Mockler\n\nFrom Hacks to Habits ‚Äî Tools & Tricks That Changed the Game\n\n\n\n\n6. Implementation / Perspectives of CROs\n\nChairs: Douglas Henry, Steven Tan\n\nExpanding the Pharmaverse beyond Clinical Reporting"
  },
  {
    "objectID": "index.html#ai-in-pharma-1",
    "href": "index.html#ai-in-pharma-1",
    "title": "R/Pharma Summit",
    "section": "AI in Pharma",
    "text": "AI in Pharma\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nCurrent Use Cases\n\nDevelopment Support: Chatbots, ADM, Adam TLF coding standards, general coding support and automation\nSpecification Generation: Potential for generating specifications from SAPs and study documents to complete the code generation loop\nClinical Operations:\n\nImproving operational efficiency\nSignal detection for safety monitoring\n\n\n\n\nImplementation Strategy\n\nBreak problems into manageable pipelines\nStart with proof-of-concepts (POCs)\nLeverage domain knowledge as a key asset\n\n\n\nTechnical Challenges\n\nData Privacy: Major concern with handling sensitive clinical data securely\nQuality Control: Issues with traceability, repeatability, and quality control\nAI Limitations: Hallucination problems with AI outputs\nImplementation Issues: Tendency to over-engineer solutions rather than creating manageable ones\n\n\n\nOrganizational Barriers\n\nManagement Resistance: Leadership not being on board due to established habits\nROI Challenges: Difficulty demonstrating return on investment\nValidation Requirements: Need for repeatability and validation processes\nTraining Needs: Comprehensive user training required for process changes\nJob Security Concerns: Worries about AI-related job displacement\n\n\n\nRegulatory and Compliance Issues\n\nAudit Trail: Maintaining proper audit trails\nDocumentation: Ensuring complete documentation and transparency of AI pathways\nQuality Standards: Meeting industry quality standards\nData Governance: Implementing proper guardrails and controls on sensitive data\n\n\n\n\n\n\n\n\n\n\n\nSource Transcript\n\n\n\n\n\nOkay, so we discussed a couple of topics in this. One of them, I‚Äôll go one by one, the usability where we are currently using. We talked about using it for chatbots, for ADM, Adam TLF coding standards, general coding support and automation. I think there was somebody who also asked if we could even generate specifications from SAPs and study documents so that you could complete the loop like, you know, specs as well as code generation. We talked about where we are using it in clinical operations. One, I think there‚Äôs somebody who is using it to improve operational efficiency and another person also indicated that they are using it for signal detection for safety monitoring. In terms of implementation strategy, we wanted to break these problems into manageable pipelines, start with POCs and leverage domain knowledge as a key asset. You want to take the next one? Thank you. And some of the issues and other barriers that were discussed, in terms of the technicality, like data privacy came up a lot and how do you handle sensitive clinical data from not, you know, falling off? And there was also traceability, repeatability, and how do you control the quality? And hallucination of the AI itself was also mentioned. And this other aspect of, I guess, in terms of implementation, people trying to wipe code into producing a manageable solution which doesn‚Äôt seem to work. So these are some of the technical implementation issues that were discussed. In terms of the organizational hurdles, broader implementation like management not being on board just because of the habits they are used to and the return of investment challenge, validation, repeatability as well. And there is also a need for comprehensive user training when especially when trying to implement a process change using AI. There‚Äôs also concerns about job displacement due to AI that were mentioned. And finally regulatory and compliance like maintaining an audit as well as ensuring complete documentation and transparency of pathways. And how do you ensure it‚Äôs meeting the industry quality standards? And finally implementing proper guardrails and controls on sensitive data in terms of the regulatory where some of the other issues discussed."
  },
  {
    "objectID": "index.html#quality-validation-in-2025",
    "href": "index.html#quality-validation-in-2025",
    "title": "R/Pharma Summit",
    "section": "Quality & Validation in 2025",
    "text": "Quality & Validation in 2025\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nShiny-Centric Validation Discussion\n\nDynamic Validation: Challenges validating Shiny apps with multiple inputs and dynamic UI changes\nCross-Language Validation: Interesting approach using different languages (e.g., SAS) to validate Shiny outputs\nUser-Centric Testing: Need for dynamic validation of user interactions beyond business logic validation\n\n\n\nPackage Validation Approaches\n\nEnvironment Types: Companies building both exploratory and validated GXP environments\nValidation Cycles: Range from every 6-12 months to agile approaches completing validation in hours\nSpeed vs.¬†Compliance: Demonstration that speed and compliance can coexist with proper processes\n\n\n\nInfrastructure Management\n\nContainer Strategy:\n\nManaging multiple packages and images is challenging\nBase images specific to projects or therapeutic areas recommended\nKubernetes as standard for container management\nContainer sizing remains technical challenge\n\nImage Management: Need for structured approach to container proliferation\n\n\n\nPackage Transparency Benefits\n\nOpen Source Advantage: Unlike SAS, can see how bugs are identified and resolved\nVisibility: Access to bug reports and fixes provides better understanding than closed-source alternatives\nInternal Package Standards: Internal packages should follow same validation processes as external packages\n\n\n\nTesting Standards and Practices\n\nSeparation of Roles: Testers should be different from developers\nValidation Definitions: No industry standard for what ‚Äúvalidated package‚Äù means - companies define their own criteria\nReproducibility: Focus on reproducible code as foundation for validation\n\n\n\nGenAI Integration Challenges\n\nOutput Validation: GenAI outputs need reproducible code that can be tested and validated\nDocumentation Transparency: Suggestion to identify AI-generated documentation for additional scrutiny\nAI-Assisted Review: Mixed opinions on using AI bots to evaluate tests - could be useful but may reduce human vigilance\n\n\n\nEmerging Tools and Technologies\n\nTesting Innovation: New automated testing tools with reporting and screenshots coming from vendors\nShinyTest Evolution: Improvements in Shiny testing capabilities\nTeal vs.¬†ShinyMeta: Teal‚Äôs approach to reproducible scripts gaining more adoption than ShinyMeta\nDuckDB Integration: Promising for handling large data in Shiny applications without full database overhead\n\n\n\nBackward Compatibility\n\nR Perception vs.¬†Reality: Addressing misconceptions about R‚Äôs backward compatibility\nDocumentation Needs: Importance of documenting package versioning and major changes\n\n\n\n\n\n\n\n\n\n\n\nSource Transcript\n\n\n\n\n\nvery much a Shiny-centric kind of spin to our validation discussion, where we had some interesting use cases about how Shiny, of course, you can set a lot of inputs and make dynamic changes in your UI of whatever table or plot you‚Äôre making. But then when you want to actually validate the results of those selections, there were some interesting cases where you might use a different even language in and of itself to do that, such as SAS, to look at how those outputs are being generated. So that was an interesting idea I never really thought about. But it also underscored the importance of, it‚Äôs one thing just to do the business logic kind of validation. I talked about my morning session. But you do need that kind of user-centric, dynamic look at what outputs you‚Äôre generating and making sure you‚Äôre on track with validating those. So I think there‚Äôs some opportunity to learn from each other on that side of it, too. And then, actually, you, Phil, you had a great comment about the number of conferences, some of the stigma that R has is a, quote unquote, lack of backward compatibility, which I personally wonder where that‚Äôs coming from, especially what BASE are, because they are very rigid to changing. But there are, obviously, things you need to consider with how you manage, say, your package versioning and documenting whenever big changes are occurring there, too. And then trying to think back on whatever parts we talked about. I think we talked about ShinyTest. Oh, yeah. And we also talked about ways to do testing in Shiny, specifically with ShinyTest, too. And there was some interesting new tooling that‚Äôs coming out from our friends at the Taurus, it sounds like, coming up very soon with augmenting that with automated reporting and screenshots that we‚Äôre going to be eager to see as we go forward with that side of it, too. One last thing, we talked about ShinyMeta and Teal. Yep. And then, yeah, exactly. So then Teal came up quite a bit, as we often see, that‚Äôs a great use case to get buy-in from an organization to leverage Shiny in the clinical area. But then you think about how they are approaching creating those reproducible scripts that you can get to, in essence, carry out what you did in the Teal app. And they went away from the ShinyMeta route that, Rosie, who had been at our farm before, that package from Posit actually was born from conversations with some of us with the Shiny team back then. But it never really got as much adoption as I think maybe it was anticipated. So we are interested to see the way Teal conducts this, if that‚Äôs a pattern that ends up being more standard in the future, always learning from what‚Äôs working well there and maybe where that can go in the validation discussion when you think about teams that say, oh, you did that great analysis in that app, but I need to be able to run that in a month from now or even a year from now, maybe to answer a question. So we do see some intriguing ideas that Teal has pioneered with getting those scripts out there. And last, but certainly not least, there were some questions about how to handle larger data in apps. And myself and others were quick to literally mouth off. DuckDB is a very promising approach so that you can have that within that environment database feature, but yet you‚Äôre not overhead of a Postgres or even NoSQL database that often came up, too. But we think DuckDB and Shiny is a great combination for big data analytics that I think will play nicely in Noveltoon in the future.\nSo we were talking mostly about package validation during both validation roundtables, and there are companies that they already built both the exploratory and the validated GXP environments, and there are a lot of companies that are just building the GXP environments, and the cycles and the processes vary, and there could be an approach where the validation cycle takes, that happens every six or even every 12 months. However, there is one company that takes more agile and fluid approach where the package validation can be completed in just a few hours, and I think that demonstrates that both speed and compliance can coexist in a proper process. And we discussed that managing the infrastructure actually is a challenge with a lot of packages and a lot of images being created, and that a good approach that can be taken in the future is to create a base image that can be specific per project or specific for a therapeutic area, and this is a container, and Kubernetes seem to be the standard for managing those. And however, container sizing continues to present a technical challenge. What else? We also discussed that the transparency for our packages packs, that this is actually a positive thing, because with SAS you don‚Äôt really know, you only know about your own bag that you submitted, but you don‚Äôt know about the other bags, and they for sure exist, and with the open source you get access, you see what happens, and you see how those bags are resolved, so we thought that this is actually a good thing. And we also agreed that internal packages, they should follow the same process as external packages, so there needs to be testing, and that actually testing, the tester should be a separate person than the developer. And we also covered a bit the topic of testing, thank you, testing the GenAI outputs, and we concluded that basically there needs to be a code that is reproducible, and this needs to be tested, and this needs to be validated as well. And I think one more thing that was quite interesting is that we don‚Äôt have a standard what the validated package means, and that every company basically defines their own way what it means that the package is validated. And regarding also the GenAI principles and validating the outputs, we thought that right now, basically we have to decide on principles regarding the GenAI, because right now we don‚Äôt state that, for example, the documentation was generated by the GenAI, and we thought that this would be valuable, because this gives additional context, and it can mean that you also would pay additional extra attention to that part, because you know that this was not generated by the human. And last but not least, we discussed whether AI bot to help evaluate if the test is good, it can be something useful, and it can be used as an extra pair of eyes. There were mixed opinions that yes, this could be useful, however, this can also make the reviewers a bit mindless, because they would rely too much on the advice and the output from the bot. Thank you."
  },
  {
    "objectID": "index.html#python-for-clinical-study-reports-and-submission-1",
    "href": "index.html#python-for-clinical-study-reports-and-submission-1",
    "title": "R/Pharma Summit",
    "section": "Python for Clinical Study Reports and Submission",
    "text": "Python for Clinical Study Reports and Submission\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nIndustry Heterogeneity\n\nMedical Devices vs.¬†Pharma: Different use cases and adoption patterns\n\nMedical device engineers more familiar with Python as standard language\nCreates challenges managing multiple languages within organizations\nSome companies choosing Python-only approach despite challenges\n\n\n\n\nPrimary Use Cases\n\nMedical Devices:\n\nLarge datasets from device data\nProduct development beyond just data analysis\nEngineering-driven adoption\n\nPharma Applications:\n\nETL processes for speed advantages\nMachine learning implementations\nData engineering tasks\n\n\n\n\nValidation Challenges\n\nR Validation Hub Principles: White paper was meant to be language-agnostic, principles apply to Python\nTrusted Sources: Questions about what constitutes trusted sources in Python ecosystem\n\nAnaconda, PyPI with RSPM scrutiny mentioned as possibilities\nPython ecosystem seen as ‚Äúwild west‚Äù compared to R\n\nStatistical Packages: Python statistical libraries still developing; roughly 8 years behind R in maturity\nContainers: Discussion of containerization for validation approaches\n\n\n\nRegulatory and Submission Status\n\nSubmission Gap: No Python submissions to regulators yet (unlike R which has established precedent)\nRegulatory Acceptance: Need for groundwork similar to what was done for R submissions\nPilot Programs: Reference to submission pilots (pilot 15) for future development\n\n\n\nPython vs.¬†R Considerations\n\nGenAI Integration: Python more prevalent in generative AI development, potentially driving adoption\nLarge Dataset Handling: Python advantages for genetic datasets and other large-scale data due to memory limitations in R\nStatistical Maturity: R still ahead in statistical package validation and regulatory acceptance\n\n\n\nFuture Directions\n\nNeed for Python-specific validation frameworks\nDevelopment of regulatory submission pathways\nBalancing language choice with organizational capabilities\n\n\n\n\n\n\n\n\n\n\n\nSource Transcript\n\n\n\n\n\nSo a couple of things, I‚Äôm going to start with a question that we had more towards the end because what is seen from the table that we were is that there is some heterogeneity on the usage of it. Some people are actually in the medical device space, some people are in the pharma space and for medical devices, a lot of the people that have used Python are engineers and they are more familiar to this, which end up becoming very much their standard language for them to use, which created an interesting thing. It becomes very difficult to manage multiple languages sometimes and in this particular case, this company has decided that they are just using Python even though there are some challenges in that space. We have a couple of different topics that we were discussing. One of them is regarding the use cases. So of course for medical devices, the use cases are very different than what‚Äôs in pharma. The kind of data that is coming from devices, large data sets, kind of forces the use of Python in a different way. There is also a little bit of a medical device space. You‚Äôre also thinking not only about the data analysis but an actual product that‚Äôs going to be created in the end after the data analysis, which is more difficult to be developed in R, in that space. But I think that one of the things that drives some of the use cases, especially for pharma, is speed, and because of that, a lot of ETLs, maybe machine learning, data engineering is really the space where pharma is looking into the usage of Python. Of course, another team was validation, that James already mentioned a little bit about this. What do we do about validation of Python? I think when the R validation hub created the white paper, even though it‚Äôs an R paper, it was meant to be language agnostic. We did put a lot of stuff that was there because that‚Äôs what we were working on, but we talked about it. Should we just make this language agnostic at the time? We decided not, but I think the principles are the same. Of course, it does raise a lot of different questions because there are assumptions in the R validation white paper that we do for R that are a little maybe easier to get from evidence perspective. For example, what‚Äôs a trusted source? And what is a trusted source in Python? I think that we heard in the table that things coming from Anaconda or maybe PyPy with maybe some RSPM may be reasonable, but also that you have to put some scrutiny around this because it‚Äôs not, again, Python is more of a wide west in comparison to R. There‚Äôs just so much more stuff going on and we have to have all that in consideration. An interesting comment, the statistics in Python, I mean, packages or libraries that are doing statistics, I think that what we heard from one person, one company, is that the groundwork to make sure that those statistical analysis is being vetted, it‚Äôs still a little early. It‚Äôs like R was like eight years back maybe, so there‚Äôs a little bit of that going on and that‚Äôs in the space for Python. There‚Äôs some discussion in the validation about the use of containers and a couple of other questions that you have that we posed in the end was Python and GenAI, so as you see, there‚Äôs a lot of GenAI, when I talk about GenAI, there‚Äôs a lot of stuff that‚Äôs going on in Python just because it‚Äôs a more used language and that may be driven or driving some of the, where the languages are going. There is some conversation about Python versus R and one thing that R is definitely ahead is the fact that submission and regulatory acceptance is already being developed, so no one has really got a Python submission yet, as far as I know, and that is groundwork that has to be done. Submission, pilot number 10, pilot 15, Eric, pilot 15, Eric, for you, and I guess the other thing is really in regards to the limitations that may be in R for certain types of data sets, gene data sets, genetic data sets and so forth because of the size of the data. Did I, is there something else that I, okay, so that‚Äôs it. Thank you."
  },
  {
    "objectID": "index.html#sces-in-2025-1",
    "href": "index.html#sces-in-2025-1",
    "title": "R/Pharma Summit",
    "section": "SCEs in 2025",
    "text": "SCEs in 2025\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nSCE Components (5 Core Elements)\n\nIDE: Development environment interface\nFile and Version Management: Source control and file handling\nData and Access: Data storage and access management\nGovernance: Policies and compliance frameworks\n\nCompute: Processing and computational resources\n\n\n\nScope and Architecture Challenges\n\nHistorical Separation: Legacy approach had separate SCEs for GXP work vs.¬†everything else\nUnified Direction: Most companies moving toward single SCE for all work\nUser Diversity: Modern SCEs serve PK, Biostatistics, Stats Programming, and other groups on same platform\n\n\n\nVendor Considerations\n\nEcosystem Promises: Vendors offer packaged solutions but create vendor lock-in concerns\nMissing Integrations: Gaps in vendor offerings, sometimes strategically maintained\nCost Management: Total cost of ownership concerns when using multiple vendors\nIntegration Overlap: Redundancy between different vendor products increases costs\n\n\n\nValidation Challenges\n\nTimeline: Validation processes take months (up to 3 months reported)\nTest Suite Maintenance: Significant burden maintaining test suites as platforms change\nIDE Validation Debate: Whether to validate IDE as part of environment or keep separate\nSystem vs.¬†Process: Best practice is qualifying systems while validating processes\n\n\n\nTechnology Evolution\n\nPython Integration: Coming into clinical reporting, mainly for transformations\nPython Validation Gap: Lacks standardized documentation and testing frameworks compared to R\nData Format Shift: Parquet replacing SAS7bdat as standard data format\nPackage Governance: Some companies using opinionated lists of approved R packages\n\n\n\nEnvironment Management\n\nUpdate Cycles: 3-6 months common window for base environment updates\nPackage Coupling: Most companies couple R packages with environments (some decouple)\nEnvironment Tracking: Tools to flag whether environments are managed/production-ready\nConsistency Requirements: Containers must be accessible across SCE, Shiny apps, HPC, etc.\n\n\n\nVersion Control and Data Access\n\nGit Adoption: Standard direction but significant change management challenges\nLearning Curve: Git presents training challenges for traditional SAS users\nData Access Complexity: Challenges with data access in Shiny applications\nTechnology Solutions: Denodo and AWS Data Zone mentioned for data access management\n\n\n\n\n\n\n\n\n\n\n\nSource Transcript\n\n\n\n\n\nand David‚Äôs. So we start off saying there‚Äôs three components to an SCE, and then we listed out five different components. So the first one was the IDE, there‚Äôs file and version management, data and access, the governance, and then the actual compute. In terms of scope, I don‚Äôt think we had an agreed scope for what actually is in an SCE. Historically, that might be defined by department structure, and it seemed it was kind of a common theme that in the legacy world, you had an SCE for GXP work and an SCE for everything else. And it seemed most companies want to have a single SCE going forwards. We talked a little bit about vendors and how some vendors kind of promise to package everything together into an ecosystem. But people tend to be quite a little bit afraid of that because of vendor lock-in. And also, there‚Äôs often missing integrations. And I think we were hypothesizing some of those are strategic to kind of force you to stay within the ecosystem. We, yeah, there are a couple of platforms specifically called out for that. And I think one comment that was made is there‚Äôs also that kind of concern around the total cost of ownership. So if you have to string together a whole bunch of different vendors to build your SCE, you kind of start ballooning the costs. And there‚Äôs overlap between the different vendor products. And at least in the modern SCEs, there‚Äôs often really diverse users in there, like PK, Biostat, Stats Programming, and other groups all on the same platform. Validation takes a long time. I think my own company seemed to be longest at three months to validate the SCE. But it seemed it was always a kind of a slow process. And the test suites themselves add a lot of burden just to maintain those because you have to update the test suites whenever something changes in the platform. There‚Äôs a bit of a debate whether the IDE should be part of the validation environment or should it be kept outside. So the IDE is kind of repeated across as a layer on top of your validated containers. And there was a discussion also on, do we validate a system or a process? And I think the best practice was to qualify the systems and focus validation just on the process. A few companies mentioned Python was coming in. So this is in the clinical reporting space, usually for transformations. How to validate Python is a challenge, though, because it doesn‚Äôt have a standardized documentation and testing framework like R. I think one company mentioned that they have an opinionated list of R packages. So you can‚Äôt just get any package validated. They tried to put some governance on what are the packages they‚Äôre validating. Parquet seems to have replaced SAS7bdat as the data format within modern SCEs. For environment management, either three or six months seems to be a common window to update the base environments. A couple of companies decoupled the R package from the environment, but that doesn‚Äôt seem to be the norm. One company mentioned that they have a tool. So if the users are kind of interactively modifying their environments, it can flag whether that environment is one of the managed ones for a production run and could also tell if it was an upcoming planned environment, which I thought was a cool thing to open source if that company was interested. And it was really important to think about consistency of environments. So if you have a container in the SCE, the Shiny app should also be able to access that container. The HPC should be able to access that container and the like. And the last two topics, one of them was Git for version control. It seemed that was the direction most companies are going in, but we really had to acknowledge there was a change management issue there with the learning curve. And we discussed some of the complications around data access, particularly in Shiny. So kind of passing through the viewer of the application‚Äôs data access. And there was mention of a couple of technologies to help. One was Denodo, and also AWS‚Äôs data zone as a way of kind of putting a middle layer to help manage that. Thank you."
  },
  {
    "objectID": "index.html#open-source-change-management-1",
    "href": "index.html#open-source-change-management-1",
    "title": "R/Pharma Summit",
    "section": "Open-Source Change Management",
    "text": "Open-Source Change Management\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nThe Foundation: Why Before How\n\nRisk Management: Change management is fundamentally about managing risk\nBreaking the IBM Rule: ‚ÄúNo one gets fired for buying IBM‚Äù - need to show upsides of change to overcome conservative mindset\nValue Proposition: Emphasize collaboration benefits, AI integration opportunities, and modern capabilities\n\n\n\nThree Pillars of Change Management\n\nPeople\n\nGrassroots vs.¬†Top-Down: Grassroots efforts have limits; executive support and repeated messaging from leadership essential\nInnovation Teams: Trend toward creating dedicated innovation groups rather than making change management a side job\nHiring vs.¬†Training:\n\nJust-in-time training more effective than advance training\nTrend toward hiring R/Python programmers from day one\nRemoving SAS requirements from job descriptions\n\nStandards Influence: Good opportunity to influence internal standards and reset limitations from legacy tools\n\n\n\nTechnology\n\nGit Foundation: Essential foundation for open source adoption and AI tool integration\nEnvironment Management: Focus on Statistical Computing Environment (SCE) setup\nContainerization: Important for managing package versioning issues (R+ENV, Docker)\n\n\n\nProcess\n\nVersion Control: Git as key process improvement\nReporting Innovation: Opportunities to rethink clinical trial reporting (e.g., using Shiny to reduce static TLFs)\nCommunication Strategy: Right people delivering right messages, expect pushback, avoid mandate-style approaches\n\n\n\n\nKey Success Factors\n\nR Validation Hub: Valuable resource for demonstrating process and risk metrics\nSpecific Use Cases: Show concrete examples of complex tables/graphs generated easily\nEnvironment Comfort: Ensure users feel comfortable in the new environment\nPackage Management: Education about dynamic nature of open source vs.¬†traditional single-package updates\nTrial Periods: Implement trials before major investments\n\n\n\n\n\n\n\n\n\n\n\nSource Transcript\n\n\n\n\n\nopen source change management: I think the first thing when it comes to change management is the why before we actually talk about the how. And I think it‚Äôs important to keep the perspective of change management is about risk. And there‚Äôs a saying that I did not make up even though no one at my table had heard about it. You can‚Äôt get, no one has ever gotten fired for buying IBM. And that saying means that like, if everyone does things a certain way, no one‚Äôs gonna get in trouble if something happens with like that way. And so you‚Äôre asking people to change the way they do analysis. And so management and people who aren‚Äôt like in this room basically see the downsides pretty obviously, but then your job as sort of thought leaders is to show them the upsides. And I think a big one is mentioned stuff like collaboration, AI, like I think AI is like oregano, everyone wants to sprinkle AI on everything. So mission AI and those things, and that‚Äôll help to make the conversation a little bit about like, yes, you see the downsides, but moving more towards the upsides. I think the R-validation hub is a big, big benefit. I think you can show that there is a process, you can show stuff like risk metric, and that‚Äôll sort of help to ameliorate some of the concerns about change, but it won‚Äôt eliminate them. When it comes to actually making the change, I think we talked a little bit about training, either buying it or doing it yourself. I think one thing to keep in mind is that when it comes to training, the best training is the training that you do yourself. So I think you need to find the right people to help make that change. I think a big thing about, especially open source change management, is environment, and I‚Äôm talking about like SCE stuff, James already solved that, so you‚Äôre fine, but work on your environment, make sure that people feel comfortable in that environment, especially if there are ways to sort of, before you make a big investment, do some kind of trials with that. For change management, communication is key. Make sure you have the right people, give the right message. I think a lot of people push back if you‚Äôre like, you have to change, you have to do it this way now. Expect some pushback. And then one of the biggest things that is a change from closed source analysis to open source is a lot of stuff about package issues. A lot of companies, they just have like one, one single sort of package that gets updated every couple of years. Obviously, this open source is much more dynamic, and so you need to maybe explain a little bit more how like software works, because if there‚Äôs an error, it could be that your code is fine, but maybe you have a, maybe the version needs to be updated, and explaining stuff like packaging, and I think a big part of that is containerization, either RAM for Docker or something, I think that‚Äôll make that change process a lot easier.\nSo we have the, for our roundtable, we‚Äôre talking about open source change management, and then we try to tackle that problem from the three pillars of change management, people, technology, and process. And for the people side, we talk about to get the momentum started, oftentimes we need very specific use cases showing that by using our Python, we can generate this very complicated tables and the graph is very easily. And then we talk about that, actually grassroot effort can only get us to a certain level and at the end of the day, we really need kind of top down decisions and then those incentives coming from the leadership team and also having those messages repeatedly like from the leadership team. And we see a trend that from mid-size and large companies, like there are creation of independent innovation groups. So we acknowledge that if like change management is a side job of someone‚Äôs, like in complimentary to someone‚Äôs day job of filing, et cetera, it‚Äôs always getting deprioritized. So we see a trend of creating those innovation teams. And we talk a little bit about hiring versus training. And we talk about for internal training, we want to have those just in time training. We see many cases that we train people, they go back to SAS work and they forgot about the training. And it seems like there is a trend that for many companies, they start to remove SAS in their job description. And we see that people are, for younger generation, we are hiring our Python programmers from day zero. And then in terms of another thing I found very interesting that in terms of people influence, we also have some discussion about this is also a good time to influence our internal standard group because there are many trivial decisions in internal standard or limitations or historical reasons from like legacy tools or legacy technology limitations. So it‚Äôs a good timing to influence them and then to kind of like reset the baseline a little bit. In terms of technology, we spend quite some time talking about Git, kind of double click on the conversation this morning. Like it‚Äôs absolutely the foundation of adoption of open source languages and also the AI tools. In terms of process, we talk about Git again, and we also talk about there are also unique opportunity for us to rethink about our clinical trial reporting process. For example, we see that in some companies, like by using Shiny, actually we can reduce number of TLGs or even get rid of the generation of static TLF for a certain reporting event. So I think that there is an opportunity to rethink what really makes sense for clinical trial reporting."
  },
  {
    "objectID": "index.html#implementation-perspectives-of-cros-1",
    "href": "index.html#implementation-perspectives-of-cros-1",
    "title": "R/Pharma Summit",
    "section": "Implementation / Perspectives of CROs",
    "text": "Implementation / Perspectives of CROs\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nCurrent Industry Landscape\n\nSAS Dominance: SAS remains a major part of CRO workflows and processes\nInnovation Gap: Differentiation between innovative CROs (smaller, forward-thinking) and traditional CROs (larger, dominant market players)\n\n\n\nKey Challenges for CROs\n\nUpskilling Requirements: Need to invest in training staff on new technologies while maintaining SAS capabilities\nDual Technology Management: Challenge of managing both traditional and modern toolchains\nGuidance Dependency: Many CROs seeking more direction from sponsor companies rather than self-directing change\nBusiness Model Constraints: Traditional ‚Äúorder taker‚Äù model doesn‚Äôt align well with consulting-style technology guidance\n\n\n\nCRO as Resource vs.¬†Bottleneck\n\nBottleneck Risk: CROs can become bottlenecks when trying to figure out technology transitions independently\nValuable Resource Potential: CROs have extensive cross-client experience that could be leveraged for best practices\nService Evolution: Pivoting from process-focused to more consultative roles as clients demand technology expertise\n\n\n\nSponsor-CRO Relationship Dynamics\n\nTimeline Expectations: Question whether sponsors understand the time required for upskilling and technology changes\nCommunication Gap: Need for better understanding between sponsors and CROs regarding transition requirements\nBusiness Model Evolution: Traditional expectations may need adjustment for new technology paradigms\n\n\n\nCommunity Engagement\n\nInformation Access: Importance of CROs participating in organizations like R/Pharma for knowledge sharing\nEcosystem Integration: CROs are significant players in the overall life sciences technology ecosystem\nIncreased Participation: Need for more CRO involvement in industry technology discussions\n\n\n\n\n\n\n\n\n\n\n\nSource Transcript\n\n\n\n\n\nSo yeah, we had a good discussion. I‚Äôm going to differentiate probably between the more innovative CROs that I think are here versus the more traditional groups that tend to dominate the industry a lot but are obviously a big part of what‚Äôs going on. So the consensus view is that there‚Äôs a big challenge in the CRO space because one, SAS is still a big part of the workflow and the process that everybody is working through and dealing with. And so the requirements to upskill people and to change that paradigm, it falls a bit on the CRO. And it‚Äôs a decision that they have to make in terms of how they‚Äôre looking at the market. And so I think a lot of these groups are looking for maybe more guidance from the sponsor side or perhaps sponsors that are more set on a certain path as opposed to expecting them to manage both. And in that sense, they‚Äôve become a bit of a bottleneck because they‚Äôre trying to figure it out on their own but they don‚Äôt always know where to go in some of these strategies. I think the other thing is though that we did talk about on the other side of that, CROs also can be a really great resource for organizations that are looking to make this transition because they have a lot of internal experience working on a lot of different studies and projects across a lot of different clients. So they‚Äôre not often set up to be consultants. They‚Äôre really more kind of order takers or process-focused organizations. But they‚Äôre pivoting a bit to try to support more of those activities as clients are demanding it of them. Yeah, I mean, there was a lot of discussion around do sponsors understand that the upskilling and the change is taking time and that there are certain requirements that need to be dealt with? And I think there‚Äôs a question there as to whether a lot of sponsors understand that. But it also just kind of fits in with the business model and the expectations. The other big, I guess the other big topic everybody discussed is the importance of being part of an organization like this because there‚Äôs a lot of information available to people as they‚Äôre trying to work through some of these topics. And perhaps an emphasis of getting more CROs involved in this discussion could be pretty useful because they are a pretty big player in the overall kind of ecosystem that we all work in. Thank you."
  }
]